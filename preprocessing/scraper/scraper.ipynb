{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIS-Data Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_YEAR = 1967\n",
    "END_YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ski-DB parser\n",
    "\n",
    "**[Website](https://www.ski-db.com/)**\n",
    "\n",
    "**[Profiles](https://www.ski-db.com/db/profiles/beat_feuz_sui_511383.php)**, of the form `https://www.ski-db.com/db/profiles/<id>.php`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeasonURL(year, isMen=True):\n",
    "    \"\"\"Get the Ski-DB page for a particular season.\n",
    "    \"\"\"\n",
    "    if not START_YEAR <= year <= END_YEAR:\n",
    "        raise ValueError(f\"An invalid WC year was given (from {START_YEAR} to {END_YEAR}).\")\n",
    "    gender = 'm' if isMen else 'f'\n",
    "    endSeason = year % 100\n",
    "    startSeason = (year-1) % 100\n",
    "    return f\"https://www.ski-db.com/db/{endSeason:02d}/cal_{gender}{startSeason:02d}{endSeason:02d}.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRacesURLs(year, isMen=True):\n",
    "    \"\"\"Get all Ski-DB pages for the races of a particular season.\n",
    "    \"\"\"\n",
    "    seasonURL = getSeasonURL(year, isMen)\n",
    "    print(f\"[INFO] Getting races URLs for the {year} season\")\n",
    "    page = urllib.request.urlopen(seasonURL)\n",
    "    soup = BeautifulSoup(page)\n",
    "    races = soup.findAll(\"tbody\", {\"class\": \"skidb\"})\n",
    "\n",
    "    raceURLs = []\n",
    "    for race in races:\n",
    "        raceURLs.append(race.findAll('a')[0]['href'])\n",
    "    \n",
    "    return raceURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRace(raceURL):\n",
    "    page = urllib.request.urlopen(raceURL)\n",
    "    soup = BeautifulSoup(page)\n",
    "    raceDB = {}\n",
    "    \n",
    "    titles = soup.findAll(\"h1\")\n",
    "    if len(titles) < 2:\n",
    "        print(f\"[WARN] No metadata for {raceURL}\")\n",
    "    else:\n",
    "        metadata = titles[1].text.split(' ')\n",
    "        raceDB['date'] = ''.join(metadata[0].split('-')[::-1]).strip()\n",
    "\n",
    "        locationMetadata = ' '.join(metadata[1:])\n",
    "        raceDB['venue'] = locationMetadata.split('[')[0].strip()\n",
    "        raceDB['country'] = locationMetadata.split('-')[1].strip().split('\\n')[0].strip()\n",
    "        raceDB['event'] = locationMetadata.split('\\n')[3].strip()\n",
    "    \n",
    "    skiers = soup.findAll(\"tbody\", {\"class\": \"skidb\"})[0].findAll(\"tr\", {\"class\": [\"blanc\", \"alt\"]})\n",
    "    raceDB['results'] = {}\n",
    "    for idx, skier in enumerate(skiers):\n",
    "        info = skier.findAll(\"td\")\n",
    "        skierDB = {}\n",
    "        skierDB['rank'] = info[0].text.strip()\n",
    "        skierDB['name'] = info[1].text.strip()\n",
    "        skierDB['country'] = info[2].text.strip()\n",
    "        skierDB['data'] = [i.text.strip() for i in info[3:] if i]\n",
    "        skierDB['id'] = skier.findAll(\"a\")[0]['href'].split('/')[-1].split('.')[0]\n",
    "        raceDB['results'][idx] = skierDB\n",
    "    \n",
    "    return raceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveJSON(data, filename):\n",
    "    with open(f'../data/{filename}.json', 'w+') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSkiDB(isMen=True):\n",
    "    db = {}\n",
    "\n",
    "    for year in range(START_YEAR, END_YEAR+1):\n",
    "        yearDB = {}\n",
    "        raceURLs = getRacesURLs(year, isMen)\n",
    "        \n",
    "        for idx, raceURL in enumerate(raceURLs):\n",
    "            yearDB[idx] = parseRace(raceURL)\n",
    "        db[year] = yearDB\n",
    "        saveJSON(db, \"wcm\" if isMen else \"wcf\")\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men's Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_m = parseSkiDB(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ski-db.com/db/87/cal_m8687.php\n",
    "berlinYear = 1987\n",
    "berlinRace = 13\n",
    "\n",
    "db_m[berlinYear][berlinRace]['date']   = \"19861228\"\n",
    "db_m[berlinYear][berlinRace]['venue']  = \"Berlin\"\n",
    "db_m[berlinYear][berlinRace]['country'] = \"GER\"\n",
    "db_m[berlinYear][berlinRace]['event']  = \"Parallel\"\n",
    "\n",
    "saveJSON(db_m, \"wcm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB:\n",
    "\n",
    "- KitzbÃ¼ehl present 2 times in 19xx on 12/01/199** (?), date parsed wrongly\n",
    "- Patrick Thaler disqualifed during Adelboden Giant Slalom 1999 (1999-01-12), not 24th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Women's Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_f = parseSkiDB(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error handling**\n",
    "\n",
    "- False time for 2nd run of Elisabeth Goergl, 2010-01-16, maribor, GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.ski-db.com/db/83/918308wc.php\n",
    "tremblantYear = 1983\n",
    "tremblantRace = 22\n",
    "\n",
    "db_f[tremblantYear][tremblantRace]['date']    = \"19830305\"\n",
    "db_f[tremblantYear][tremblantRace]['venue']   = \"Mont Tremblant\"\n",
    "db_f[tremblantYear][tremblantRace]['country'] = \"CAN\"\n",
    "db_f[tremblantYear][tremblantRace]['event']   = \"Downhill\"\n",
    "\n",
    "# https://www.ski-db.com/db/83/938303wc.php\n",
    "tremblantRace = 23\n",
    "\n",
    "db_f[tremblantYear][tremblantRace]['date']    = \"19830306\"\n",
    "db_f[tremblantYear][tremblantRace]['venue']   = \"Mont Tremblant\"\n",
    "db_f[tremblantYear][tremblantRace]['country'] = \"CAN\"\n",
    "db_f[tremblantYear][tremblantRace]['event']   = \"Giant Slalom\"\n",
    "\n",
    "## The following have weird errors due to an error in the HTML\n",
    "# https://www.ski-db.com/db/09/91200903wc.php\n",
    "data = db_f['2009']['25']['results']['25']['data']\n",
    "data = data[:3]\n",
    "data[1] = data[1].split(' ')[0]\n",
    "data[2] = ''\n",
    "db_f['2009']['25']['results']['25']['data'] = data\n",
    "\n",
    "# https://www.ski-db.com/db/10/91201006wc.php\n",
    "data = db_f['2010']['24']['results']['27']['data']\n",
    "data = data[:3]\n",
    "data[1] = data[1].split(' ')[0]\n",
    "data[2] = ''\n",
    "db_f['2010']['24']['results']['27']['data'] = data\n",
    "\n",
    "saveJSON(db_f, \"wcf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "1967-2003: No ski brands\n",
    "\n",
    "2003-2020: Ski brands as last index or data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/wcm.json\") as f:\n",
    "    db_m = json.load(f)\n",
    "with open(\"../data/wcf.json\") as f:\n",
    "    db_f = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLine(db, season, date, venue, country, event,\n",
    "            ath_rank=0, ath_name='', ath_country='',\n",
    "            ath_time_run_1=0, ath_time_run_2=0,\n",
    "            ath_time=0, ath_time_diff=0, ath_ski='', ath_id=''):\n",
    "    db[\"season\"].append(season)\n",
    "    db[\"date\"].append(date)\n",
    "    db[\"venue\"].append(venue)\n",
    "    db[\"country\"].append(country)\n",
    "    db[\"event\"].append(event)\n",
    "    db[\"ath_rank\"].append(ath_rank)\n",
    "    db[\"ath_name\"].append(ath_name)\n",
    "    db[\"ath_country\"].append(ath_country)\n",
    "    db[\"ath_time_run_1\"].append(ath_time_run_1)\n",
    "    db[\"ath_time_run_2\"].append(ath_time_run_2)\n",
    "    db[\"ath_time\"].append(ath_time)\n",
    "    db[\"ath_time_diff\"].append(ath_time_diff)\n",
    "    db[\"ath_ski\"].append(ath_ski)\n",
    "    db[\"ath_id\"].append(ath_id)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTimings(toParse):\n",
    "    \"\"\"Convert all times in cs (hundredths of second).\n",
    "    \"\"\"\n",
    "    if not toParse or 'n/d' in toParse:\n",
    "        return 0\n",
    "    timing = toParse.split('.')\n",
    "    time = int(timing[1][:2])\n",
    "    timing = timing[0].split(\"'\")\n",
    "    if len(timing) > 1:\n",
    "        time += int(timing[0])*6000\n",
    "        time += int(timing[1])*100\n",
    "    else:\n",
    "        time += int(timing[0])*100\n",
    "    return int(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDB(db):\n",
    "    db_parsed = {\n",
    "        \"season\": [],\n",
    "        \"date\": [],\n",
    "        \"venue\": [],\n",
    "        \"country\": [],\n",
    "        \"event\": [],\n",
    "        \"ath_rank\": [],\n",
    "        \"ath_name\": [],\n",
    "        \"ath_country\": [],\n",
    "        \"ath_time_run_1\": [],\n",
    "        \"ath_time_run_2\": [],\n",
    "        \"ath_time\": [],\n",
    "        \"ath_time_diff\": [],\n",
    "        \"ath_ski\": [],\n",
    "        \"ath_id\": [],\n",
    "    }\n",
    "    for year in range(START_YEAR, END_YEAR+1):\n",
    "        db_season = db[str(year)]\n",
    "        season = year\n",
    "        for db_race in db_season.values():\n",
    "            date = datetime.strptime(db_race['date'], \"%Y%m%d\")\n",
    "            venue = db_race['venue']\n",
    "            country = db_race['country']\n",
    "            event = db_race['event']\n",
    "            results = db_race['results']\n",
    "            if results:\n",
    "                for athlete in results.values():\n",
    "                    rank = athlete['rank']\n",
    "                    if rank.isdigit():\n",
    "                        rank = int(rank)\n",
    "                    elif rank[0] == '.':\n",
    "                        rank = int(rank[1:])\n",
    "                    name = athlete['name']\n",
    "                    ath_country = athlete['country']\n",
    "                    data = athlete['data']\n",
    "                    \n",
    "                    ath_time = data[2] if len(data) > 3 else data[0] if len(data) > 1 else None\n",
    "                    ath_time_diff = data[3] if len(data) > 3 else data[1] if len(data) > 1 else None\n",
    "                    if ath_time_diff and ath_time_diff[1:].replace('.', '').isdigit():\n",
    "                        ath_time_diff = int(ath_time_diff.replace('.', ''))\n",
    "                    elif ath_time_diff and ath_time_diff[:2] == '+-':\n",
    "                        # Fix weird bug in Ski-DB\n",
    "                        ath_time_diff = int(ath_time_diff[2:].replace('.', ''))\n",
    "                    else:\n",
    "                        ath_time_diff = 0\n",
    "                    ath_time_run_1 = data[0] if len(data) > 3 else None\n",
    "                    ath_time_run_2 = data[1] if len(data) > 3 else None\n",
    "                    ath_ski = ''\n",
    "                    if len(data) == 5:\n",
    "                        ath_ski = data[4]\n",
    "                    elif len(data) == 3:\n",
    "                        ath_ski = data[2]\n",
    "                    db_parsed = addLine(db_parsed, season, date, venue, country, event,\n",
    "                                        rank, name, ath_country, convertTimings(ath_time_run_1), convertTimings(ath_time_run_2),\n",
    "                                        convertTimings(ath_time), ath_time_diff, ath_ski, athlete['id'])\n",
    "            else:\n",
    "                db_parsed = addLine(db_parsed, season, date, venue, country, event)\n",
    "    return db_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToDF(db):\n",
    "    df = pd.DataFrame(parseDB(db))\n",
    "    df.ath_time_run_1 = df.ath_time_run_1.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = convertToDF(db_m)\n",
    "dfm.to_csv('../data/wcm.csv', index=False)\n",
    "dfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = convertToDF(db_f)\n",
    "dff.to_csv('../data/wcf.csv', index=False)\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Note that all times are recorded as hundredths of seconds, as to not lose any precision with floating point numbers.\n",
    "\n",
    "To discuss: ranks are not integers beacause they contain information on DNFs, etc. Should we convert them  to integers and hold the extra information in another column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseCSV(isMale=True):\n",
    "    gender = 'm' if isMale else 'f'\n",
    "    df = pd.read_csv(f'../../data/wc{gender}.csv')\n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    df.date = pd.to_datetime(df.date, format='%Y-%m-%d')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = parseCSV()\n",
    "dff = parseCSV(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm['ath_rank'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athlete photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(10000)\n",
    "def getSearchSoup(firstname, lastname, gender):\n",
    "    g = 'M' if gender else 'L'\n",
    "    url = f\"https://www.fis-ski.com/DB/general/biographies.html?lastname={lastname}&firstname={firstname}&sectorcode=AL&gendercode={g}&search=true\"\n",
    "    page = urllib.request.urlopen(url)\n",
    "    return BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAthleteInfo(firstname, lastname, gender):\n",
    "    try:\n",
    "        soup = getSearchSoup(firstname, lastname, gender)\n",
    "    except:\n",
    "        print(f\"[ERR] Could not parse {firstname} {lastname}\")\n",
    "        return None, 'ENC'\n",
    "    entries = soup.findAll(\"a\", {\"class\": \"table-row\"})\n",
    "    if len(entries) == 0:\n",
    "        return None, 'ERR'\n",
    "    if len(entries) > 1:\n",
    "        return None, 'WAR'\n",
    "    info = entries[0].find('div').findAll('div')\n",
    "    return info[3].find(text=True), info[8].find(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherNames(gender):\n",
    "    g = 'm' if gender else 'f'\n",
    "    wc = pd.read_csv(f'../../data/wc{g}.csv')\n",
    "    names = [i for i in list(set(wc.ath_name.tolist())) if not isinstance(i, float)]\n",
    "    searchNames = {}\n",
    "    for n in names:\n",
    "        split_n = n.strip().split(' ')\n",
    "        searchNames[n] = split_n[0], split_n[-1]\n",
    "    return searchNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherInfo(gender):\n",
    "    searchNames = gatherNames(gender)\n",
    "    found = {}\n",
    "    error = {}\n",
    "    for name, (fn, ln) in searchNames.items():\n",
    "        id, bd = getAthleteInfo(fn, ln, gender)   \n",
    "        if id:\n",
    "            found[name] = id, bd\n",
    "        else:\n",
    "            error[name] = id, bd\n",
    "    return found, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundMale, errorMale = gatherInfo(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundFemale, errorFemale = gatherInfo(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoFound(found, error):\n",
    "    print(f'INFO: {len(found)}')\n",
    "    print(f'ERR: {sum([1 for name, (a, err) in error.items() if err == \"ERR\"])}')\n",
    "    print(f'WAR: {sum([1 for name, (a, err) in error.items() if err == \"WAR\"])}')\n",
    "    print(f'ENC: {sum([1 for name, (a, err) in error.items() if err == \"ENC\"])}')\n",
    "    \n",
    "infoFound(foundFemale, errorFemale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foundToDb(found, error):\n",
    "    ath_db = {\n",
    "        'names': [],\n",
    "        'ids': [],\n",
    "        'bds': [],\n",
    "    }\n",
    "    for name, (id, bd) in found.items():\n",
    "        ath_db['names'].append(name)\n",
    "        ath_db['ids'].append(id)\n",
    "        ath_db['bds'].append(bd)\n",
    "    for name, (id, bd) in error.items():\n",
    "        ath_db['names'].append(name)\n",
    "        ath_db['ids'].append(id)\n",
    "        ath_db['bds'].append(bd)\n",
    "    return ath_db\n",
    "\n",
    "ath_db = foundToDb(foundFemale, errorFemale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ath_db).to_csv('athf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men\n",
    "\n",
    "To manually complete:\n",
    "\n",
    "- [x] Bernard Russi: -10089, 1948 (-> Bernhard?)\n",
    "- [x] Mario Matt: 50707, 09-04-1979\n",
    "- [x] Christian Hirschbuehel: 53889, 19-04-1990\n",
    "- [x] Pirmin Zurbriggen: 510274, 1963\n",
    "- [x] Carlo Janka: 5111313, 15-10-1986\n",
    "- [x] Marc Gini: 511127, 08-11-1984\n",
    "- [x] Jean Claude Killy: -10427, 1943\n",
    "- [x] Peter Mueller: 510173, 1957\n",
    "- [x] Willi Favre: -10945, None (-> Willy?)\n",
    "- [x] Aksel-Lund Svindal: 421328, 26-12-1982\n",
    "- [x] Bruno Kernen: 510478, 1972\n",
    "- [x] David Ryding: 220689, 05-12-1986 (-> Dave?)\n",
    "- [x] Sebastian-Foss Solevaag: 422082, 13-07-1991\n",
    "- [x] others\n",
    "\n",
    "### Women\n",
    "\n",
    "- /!\\ -> Heidi Zimmermann -> Zimmerman (-10362)\n",
    "- /!\\ -> Christina Geiger -> Christine (205362)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(10000)\n",
    "def getCompetitorId(fis_id):\n",
    "    url = f\"https://www.fis-ski.com/DB/general/biographies.html?sectorcode=AL&fiscode={fis_id}&search=true\"\n",
    "    page = urllib.request.urlopen(url)\n",
    "    return BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(10000)\n",
    "def getProfileSoup(code):\n",
    "    url = f\"https://www.fis-ski.com/DB/general/athlete-biography.html?sectorcode=AL&competitorid={code}\"\n",
    "    page = urllib.request.urlopen(url)\n",
    "    return BeautifulSoup(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERR] Could not find -10866: Stefano Anzi\n",
      "[ERR] Could not find -10855: Silvano Meli\n",
      "[ERR] Could not find -10952: Yves Tavernier\n",
      "[ERR] Could not find -10871: Stig Strand\n",
      "[ERR] Could not find -10930: Walter Tresch\n",
      "[ERR] Could not find -10901: Torsten Jakobsson\n",
      "[ERR] Could not find -10890: Tino Pietrogiovanna\n",
      "[ERR] Could not find -10943: Willi Lesch\n",
      "[ERR] Could not find -10888: Tim Gilhooly\n",
      "[ERR] Could not find -10868: Steve Mahre\n",
      "[ERR] Could not find -10940: Werner Rhyner\n",
      "[ERR] Could not find 511527: Marc Gehrig\n",
      "[ERR] Could not find -10869: Steve Podborski\n",
      "[ERR] Could not find -10887: Tiger Shaw\n",
      "[ERR] Could not find -10950: Wolfram Ortner\n",
      "[ERR] Could not find -10926: Vladimir Andreev\n",
      "[ERR] Could not find -10929: Walter Gugele\n",
      "[ERR] Could not find -10939: Werner Mattle\n",
      "[ERR] Could not find -10941: Wilfried Muxel\n",
      "[ERR] Could not find -10907: Tyler Palmer\n",
      "[ERR] Could not find -10892: Todd Brooker\n",
      "[ERR] Could not find -10865: Stefan Sodat\n",
      "[ERR] Could not find 380341: Max Ullrich\n",
      "[ERR] Could not find -10342: Guy Perillat\n",
      "[ERR] Could not find -10955: Jim Heuga\n",
      "[ERR] Could not find -10936: Werner Grissmann\n",
      "[ERR] Could not find -10346: Hank Kashiwa\n",
      "[ERR] Could not find -10893: Tomaz Cerkovnik\n",
      "[ERR] Could not find -10896: Tor Melander\n",
      "[ERR] Could not find -10900: Torjus Berge\n",
      "[ERR] Could not find 290426: Josef Polig\n",
      "[ERR] Could not find -10882: Terry Palmer\n",
      "[ERR] Could not find -10861: Vladimir Spider Sabich\n",
      "[ERR] Could not find 290425: Christian Polig\n",
      "[ERR] Could not find -10937: Werner Margreiter\n",
      "[ERR] Could not find -10465: Josef Schick\n",
      "[ERR] Could not find -10949: Wolfgang Junginger\n",
      "[ERR] Could not find -10931: Walter Vesti\n",
      "[ERR] Could not find -10894: Tomaz Cizman\n",
      "[ERR] Could not find -10942: Willi Frommelt\n",
      "[ERR] Could not find -10938: Werner Marti\n",
      "[ERR] Could not find -10902: Toshihiro Kaiwa\n",
      "[ERR] Could not find -10935: Werner Bleiner\n",
      "[ERR] Could not find -10945: Willi Favre\n"
     ]
    }
   ],
   "source": [
    "athm_db = pd.read_csv('athm.csv').to_dict()\n",
    "athf_db = pd.read_csv('athf.csv').to_dict()\n",
    "ath_db_added = []\n",
    "\n",
    "for db in [athm_db, athf_db]:\n",
    "    add = {\n",
    "        \"competitor_id\": {},\n",
    "        \"photo\": {},\n",
    "        \"club\": {},\n",
    "        \"country\": {},\n",
    "    }\n",
    "    for i, id in enumerate(db['ids'].values()):\n",
    "        soup = getCompetitorId(id)\n",
    "        if not soup.find(\"a\", {\"class\": \"table-row\"}):\n",
    "            soup = getSearchSoup(db['names'][i].split(' ')[0], db['names'][i].split(' ')[-1], i == 0)\n",
    "            if len(soup.findAll(\"a\", {\"class\": \"table-row\"})) > 1:\n",
    "                soup = None\n",
    "        cid = None\n",
    "        try:\n",
    "            cid = int(soup.find(\"a\", {\"class\": \"table-row\"})['href'].split('=')[-1])\n",
    "        except:\n",
    "            print(f\"[ERR] Could not find {id}: {db['names'][i]}\")\n",
    "            cid = None\n",
    "        if cid and cid >= 0:\n",
    "            add['competitor_id'][i] = cid\n",
    "            soup = getProfileSoup(cid)\n",
    "            if soup.find(\"div\", {\"class\": \"avatar__image\"}):\n",
    "                add['photo'][i] = soup.find(\"div\", {\"class\": \"avatar__image\"})['style'].split(\"('\")[1].split(\"')\")[0]\n",
    "            add['club'][i] = soup.find(\"div\", {\"class\": \"athlete-profile__team\"}).string\n",
    "            add['country'][i] = soup.find(\"span\", {\"class\": \"country__name-short\"}).string\n",
    "\n",
    "    db.update(add)\n",
    "    ath_db_added.append(pd.DataFrame(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, db in enumerate(ath_db_added):\n",
    "    ath_db_added[i] = db[db['ids'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ath_db_added[1].to_csv('athfplus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = parseCSV()\n",
    "dff = parseCSV(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesm = [i for i in list(set(dfm.ath_name.tolist())) if not isinstance(i, float)]\n",
    "namesf = [i for i in list(set(dff.ath_name.tolist())) if not isinstance(i, float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfm = pd.read_csv(\"athmplus.csv\")\n",
    "new_dff = pd.read_csv(\"athfplus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_f = set(namesf) - set(new_dff.to_dict()['names'].values())\n",
    "countries_f = [dff[dff['ath_name'] == n].iloc[0]['ath_country'] for n in missing_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dff = new_dff.to_dict()\n",
    "new_df_size = len(dict_dff['names'])\n",
    "for n, c in zip(missing_f, countries_f):\n",
    "    dict_dff['names'][new_df_size] = n\n",
    "    dict_dff['country'][new_df_size] = c\n",
    "    dict_dff['ids'][new_df_size] = \"\"\n",
    "    dict_dff['bds'][new_df_size] = \"\"\n",
    "    dict_dff['competitor_id'][new_df_size] = \"\"\n",
    "    dict_dff['photo'][new_df_size] = \"\"\n",
    "    dict_dff['club'][new_df_size] = \"\"\n",
    "    new_df_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict_dff).to_csv(\"athffinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(\"athmfinal.csv\")\n",
    "dff = pd.read_csv(\"athffinal.csv\")\n",
    "del dfm['Unnamed: 0']\n",
    "del dff['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.to_csv(\"athm.csv\",index=False)\n",
    "dff.to_csv(\"athf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(\"athm.csv\")\n",
    "dff = pd.read_csv(\"athf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm.astype({\"ids\": pd.Int64Dtype(), \"competitor_id\": pd.Int64Dtype()}).to_csv(\"athm.csv\",index=False)\n",
    "dff.astype({\"ids\": pd.Int64Dtype(), \"competitor_id\": pd.Int64Dtype()}).to_csv(\"athf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(\"../../data/athm.csv\").to_dict()\n",
    "dff = pd.read_csv(\"../../data/athf.csv\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = {}\n",
    "for i, n in dfm['name'].items():\n",
    "    profiles[n] = {}\n",
    "    for cat in dfm:\n",
    "        val = dfm[cat][i]\n",
    "        if cat != 'name':\n",
    "            profiles[n][cat] = val if val and str(val) != \"nan\" else ''\n",
    "for i, n in dff['name'].items():\n",
    "    profiles[n] = {}\n",
    "    for cat in dff:\n",
    "        val = dff[cat][i]\n",
    "        if cat != 'name' and val and str(val) != \"nan\":\n",
    "            profiles[n][cat] = val if val and str(val) != \"nan\" else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ath.json\", '+w') as f:\n",
    "    json.dump(profiles, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athlete statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(\"../../data/wcm.csv\")\n",
    "dff = pd.read_csv(\"../../data/wcf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPoints(season):\n",
    "    def getPointsInt(season):\n",
    "        season = int(season)\n",
    "        if season < 1979:\n",
    "            return pd.read_csv(\"../../data/points67-79.csv\").to_dict()['points']\n",
    "        if season == 1979:\n",
    "            return pd.read_csv(\"../../data/points79.csv\").to_dict()['points']\n",
    "        if season < 1992:\n",
    "            return pd.read_csv(\"../../data/points80-91.csv\").to_dict()['points']\n",
    "        if season == 1992:\n",
    "            return pd.read_csv(\"../../data/points92.csv\").to_dict()['points']\n",
    "        return pd.read_csv(\"../../data/points93-now.csv\").to_dict()['points']\n",
    "    points = defaultdict(int)\n",
    "    doc = getPointsInt(season)\n",
    "    for k, v in doc.items():\n",
    "        points[str(k+1)] = v\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPoints(2020)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career\n",
    "df = parseCSV(False)\n",
    "names = [i for i in list(set(df.ath_name.tolist())) if not isinstance(i, float)]\n",
    "for name in names:\n",
    "    ath = df[df.ath_name == name]\n",
    "    ## Global points, per season, per discipline\n",
    "    ath['points'] = ath.apply(lambda row: getPoints(row['season'])[str(row['ath_rank'])], axis=1)\n",
    "    pts_raw = ath.groupby(['season', 'event'])['points'].sum().to_dict()\n",
    "    pts = defaultdict(dict)\n",
    "    for (season, event), points in pts_raw.items():\n",
    "        pts[season][event] = {'points':points}\n",
    "    pts = dict(pts)\n",
    "    nb_events = ath.groupby(['season', 'event'])['date'].count()\n",
    "    for (season, event), nb in nb_events.items():\n",
    "        pts[season][event]['n'] = nb\n",
    "    with open('data/'+'_'.join(name.lower().split(' '))+'.json', '+w') as f:\n",
    "        json.dump(pts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slalom          18123\n",
       "Giant Slalom    15757\n",
       "Downhill        14350\n",
       "Super G         10951\n",
       "Combined         2917\n",
       "Parallel          450\n",
       "Name: event, dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
