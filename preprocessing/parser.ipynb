{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from iso3166 import countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = \"wcm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rank_67_79 = pd.read_csv(f'../../data/points67-79.csv')#, index_col=0)\n",
    "df_rank_79 = pd.read_csv(f'../../data/points79.csv')\n",
    "df_rank_80_91 = pd.read_csv(f'../../data/points80-91.csv')\n",
    "df_rank_92 = pd.read_csv(f'../../data/points92.csv')\n",
    "df_rank_93_now = pd.read_csv(f'../../data/points93-now.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_translation = {\n",
    "    \"MON\": \"MCO\",\n",
    "    \"MAD\": \"MDG\",\n",
    "    \"CRO\": \"HRV\",\n",
    "    \"BUL\": \"BGR\",\n",
    "    \"ZIM\": \"ZWE\",\n",
    "    \"CKS\": \"SVK\",\n",
    "    \"DAN\": \"DNK\",\n",
    "    \"KOS\": \"XKX\",\n",
    "    \"URS\": \"RUS\",\n",
    "    \"GRE\": \"GRC\",\n",
    "    \"CHI\": \"CHL\",\n",
    "    \"SUI\": \"CHE\",\n",
    "    \"NED\": \"NLD\",\n",
    "    \"IRA\": \"IRN\",\n",
    "    \"LIB\": \"LBN\",\n",
    "    \"SLO\": \"SVK\",\n",
    "    \"LAT\": \"LVA\",\n",
    "    \"GER\": \"DEU\",\n",
    "    \"JUG\": \"SRB\",\n",
    "    \"IRE\": \"IRL\",\n",
    "    \"RSA\": \"ZAF\",\n",
    "    \"SPA\": \"ESP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha2_country(alpha3_name):\n",
    "    return getattr(countries.get(country_translation.get(alpha3_name, alpha3_name), alpha3_name), \"alpha2\", \"\")\n",
    "def get_name_country(alpha3_name):\n",
    "    return getattr(countries.get(country_translation.get(alpha3_name, alpha3_name), alpha3_name), \"name\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor Race results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"m\", \"f\"]:\n",
    "\n",
    "    df_photo = pd.read_csv(f'../../data/ath{type}.csv')[[\"name\", \"photo\"]]\n",
    "    df = pd.read_csv(f'../../data/wc{type}.csv')\n",
    "    df = df.drop(columns=[\"ath_ski\", \"ath_id\"])\n",
    "    df.country = df.country.apply(get_alpha2_country)\n",
    "    df.ath_country = df.ath_country.apply(get_alpha2_country)\n",
    "    df[\"country_name\"] = df.country.apply(get_name_country)\n",
    "    df[\"ath_country_name\"] = df.ath_country.apply(get_name_country)\n",
    "    df = df[df['ath_name'].notna()]\n",
    "    df = df.merge(df_photo, how=\"left\",left_on=\"ath_name\", right_on=\"name\").drop(columns=[\"name\"])  \n",
    "    df.to_csv(f'../../web/website/data/race_results_wc{type}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"m\", \"f\"]:\n",
    "    df = pd.read_csv(f'../../data/wc{type}.csv')\n",
    "    women_events = {str(season) :{x:[y,z] for x, y, z in df[df[\"season\"] == season][[\"date\", \"venue\", \"event\"]].drop_duplicates().values} for season in df[\"season\"].unique()}\n",
    "    import json \n",
    "    with open(f'../../web/website/data/wc{type}_events.json', 'w') as fp:\n",
    "        json.dump(women_events, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find event locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f'../../data/wcm.csv'), pd.read_csv(f'../../data/wcf.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country = df.country.apply(get_alpha2_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_cities = [(x[0], \", \".join(x)) for x in df[[\"venue\", \"country\"]].drop_duplicates().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "locations = []\n",
    "for city_name, city in events_cities:\n",
    "    address = city\n",
    "    geolocator = Nominatim()\n",
    "    location = geolocator.geocode(address, timeout=10, exactly_one=False)\n",
    "    locations.append((city_name, location[0] if location else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_location = {\n",
    "    \"Borovez\" : (42.2673025, 23.6060001),\n",
    "\"Yong Pyong\": (37.644881, 128.681255),\n",
    " \"Les Arcs\": (45.572354, 6.829653),\n",
    " \"La Mongie\": (42.909253, 0.167278),\n",
    " \"Campiglio\": (46.229577, 10.826522), \n",
    " \"Santa Caterina\": (46.391973, 10.480116),\n",
    " \"Cortina\": (46.536306, 12.121626),\n",
    " \"Kvitfjell\": (61.485798, 10.136583),\n",
    " \"Voss\": (60.634741, 6.425773),\n",
    " \"Naeba\": (36.956099, 138.756491),\n",
    " \"Mont St. Anne\": (47.117210, -70.904111),\n",
    " \"Nakiska\" : (50.942825, -115.151013),\n",
    " \"Copper Mnt.\" : (39.500336, -106.155852),\n",
    " \"Steamboat\": (40.458830, -106.804668),\n",
    " \"Jackson Hole\" : (43.587732, -110.827897),\n",
    " \"Breckenridge\" : (39.476483, -106.047852),\n",
    " \"Sun Valley\" : (43.671389, -114.367284),\n",
    " \"Garmisch\": (47.472021, 11.063838),\n",
    " \"Beaver Creek\": (39.604452, -106.516807),\n",
    " \"Crystal Mnt.\": (46.936169, -121.474640),\n",
    " \"Sugarloaf\":(45.054363, -70.308575),\n",
    " \"Sunshine\": (51.119870, -115.763976),\n",
    " \"Waterville\": (43.965456, -71.527769),\n",
    " \"Panorama\": (50.460435, -116.238179),\n",
    " \"Les Contamines\": (45.786378, 6.693766),\n",
    " \"St. Gervais\": (45.889079, 6.706937),\n",
    " \"Limone\": (44.201048, 7.577518),\n",
    " \"Villars\": (46.304778, 7.055345),\n",
    " \"Fluehli\": (46.822168, 8.030034),\n",
    " \"Aare\": (63.402575, 13.076255),\n",
    " \"Reiteralm\" : (47.388662, 13.613442),\n",
    " \"Sankt Anton\" : (47.130511, 10.268137),\n",
    " \"Badgastein\" : (47.116867, 13.139601),\n",
    " \"Val Zoldana\" : (46.391142, 12.100192),\n",
    " \"Squaw Valley\": (39.195895, -120.234871),\n",
    " \"Mammoth Mnt.\": (37.630436, -119.032434)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "lat_lon_dict = {}\n",
    "#latlon = [(location[0].latitude, location[0].longitude, city_name) for city_name, location in locations if location is not None]\n",
    "mapit = folium.Map( location=[ 45, 10 ] , zoom_start=6)\n",
    "for city_name, location in locations:\n",
    "    if city_name in name_to_location.keys():\n",
    "        print(city_name)\n",
    "        latitude, longitude = name_to_location.get(city_name,(0,0))\n",
    "    else:        \n",
    "        latitude, longitude = location.latitude, location.longitude\n",
    "    lat_lon_dict[city_name] = [latitude, longitude]\n",
    "    folium.Marker( location=[latitude, longitude ], popup=city_name.replace(\"'\",\"\") ).add_to( mapit )\n",
    "    \n",
    "mapit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('../../web/website/data/event_location.json', 'w') as fp:\n",
    "    json.dump(lat_lon_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for Bar chart race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_parallel_points(position):\n",
    "    points = {1:100,\n",
    "              2:80,\n",
    "              3:60,\n",
    "              4:50,\n",
    "              5:40,\n",
    "              9:15}\n",
    "    return points.get(position, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(season):\n",
    "    df_rank = df_rank_93_now\n",
    "    if (season < 1979):\n",
    "        df_rank = df_rank_67_79\n",
    "    elif (season < 1980):\n",
    "        df_rank = df_rank_79\n",
    "    elif (season < 1992):\n",
    "        df_rank = df_rank_80_91\n",
    "    elif (season < 1993):\n",
    "        df_rank = df_rank_92\n",
    "    else:\n",
    "        df_rank = df_rank_93_now\n",
    "        \n",
    "    return df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wf in [\"wcf\", \"wcm\"]:\n",
    "    df = pd.read_csv(f'../../data/{wf}.csv').drop(['ath_ski'], axis=1)\n",
    "    df.ath_country = df.ath_country.apply(get_alpha2_country)\n",
    "    for season in range(1967, 2021):\n",
    "\n",
    "\n",
    "        df_rank = get_points(season)\n",
    "\n",
    "        df_season = df[df[\"season\"] == season]\n",
    "        df_season = df_season.dropna()\n",
    "\n",
    "        df_season[\"ath_rank\"]=df_season[\"ath_rank\"].apply(lambda x: np.where(x.isdigit(),x,'0')).astype(int)\n",
    "\n",
    "        events = df_season[[\"season\", \"date\", \"venue\", \"country\", \"event\"]].drop_duplicates().reset_index(drop=\"True\")\n",
    "        events[\"key\"] = 0\n",
    "        skiers = df_season[[\"ath_name\", \"ath_country\", \"ath_id\"]].drop_duplicates().reset_index(drop=\"True\")\n",
    "        skiers[\"key\"] = 0\n",
    "\n",
    "        cart_prod = events.merge(skiers, how='outer')\n",
    "        df_season = pd.merge(cart_prod, df_season,  how='left', left_on=[\"season\", \"date\", \"venue\", \"country\", \"event\", \"ath_name\", \"ath_country\", \"ath_id\"], right_on = [\"season\", \"date\", \"venue\", \"country\", \"event\", \"ath_name\", \"ath_country\", \"ath_id\"])\n",
    "        df_season = df_season.fillna(0, downcast='infer')\n",
    "        df_season[\"ath_rank\"] = df_season.apply(lambda x : 0 if x.ath_rank > df_rank.ath_rank.max() else x.ath_rank, axis=1)\n",
    "        df_total = pd.merge(df_season, df_rank, on=\"ath_rank\")\n",
    "      \n",
    "        specialty = df_total.groupby([\"ath_name\", \"event\"]).points.sum().reset_index()\n",
    "        specialty = specialty.loc[specialty.groupby('ath_name')['points'].idxmax()].rename(columns={\"event\": \"specialty\"}).reset_index().drop([\"index\", \"points\"], axis = 1)\n",
    "        \n",
    "        df_total = pd.merge(df_total, specialty, on=\"ath_name\")\n",
    "        \n",
    "        df_total = df_total.sort_values(by=['date'])\n",
    "        df_total[\"value\"] = df_total.groupby(['ath_name'])[\"points\"].cumsum()\n",
    "        df_total = df_total.drop_duplicates(subset=[\"date\", \"ath_name\"], keep=\"first\")\n",
    "        df_total.rename(columns={\"ath_name\": \"name\"})[[\"date\", \"name\", \"ath_country\", \"value\", \"specialty\"]].to_csv(f\"../../web/website/data/rankings/{wf}_ranking_{season}.csv\", index=False,)\n",
    "        \n",
    "        \n",
    "\n",
    "        df_rank = get_points(season)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_season = df[df[\"season\"] == season]\n",
    "        df_season = df_season.dropna()\n",
    "\n",
    "        for event in set(df_season.event.tolist()):\n",
    "            df_season_event = df_season[df_season[\"event\"] == event]\n",
    "\n",
    "            df_season_event[\"ath_rank\"]=df_season_event[\"ath_rank\"].apply(lambda x: np.where(x.isdigit(),x,'0')).astype(int)\n",
    "\n",
    "            events = df_season_event[[\"season\", \"date\", \"venue\", \"country\", \"event\"]].drop_duplicates().reset_index(drop=\"True\")\n",
    "            events[\"key\"] = 0\n",
    "            skiers = df_season_event[[\"ath_name\", \"ath_country\", \"ath_id\"]].drop_duplicates().reset_index(drop=\"True\")\n",
    "            skiers[\"key\"] = 0\n",
    "\n",
    "            cart_prod = events.merge(skiers, how='outer')\n",
    "\n",
    "            df_season_event = pd.merge(cart_prod, df_season_event,  how='left', left_on=[\"season\", \"date\", \"venue\", \"country\", \"event\", \"ath_name\", \"ath_country\", \"ath_id\"], right_on = [\"season\", \"date\", \"venue\", \"country\", \"event\", \"ath_name\", \"ath_country\", \"ath_id\"])\n",
    "            df_season_event = df_season_event.fillna(0, downcast='infer')\n",
    "            df_season_event[\"ath_rank\"] = df_season_event.apply(lambda x : 0 if x.ath_rank > df_rank.ath_rank.max() else x.ath_rank, axis=1)\n",
    "        \n",
    "            df_total = pd.merge(df_season_event, df_rank, on=\"ath_rank\")\n",
    "            \n",
    "            df_total = pd.merge(df_total, specialty, on=\"ath_name\")\n",
    "        \n",
    "            df_total = df_total.sort_values(by=['date'])\n",
    "            \n",
    "\n",
    "            df_total[\"value\"] = df_total.groupby(['ath_name'])[\"points\"].cumsum()\n",
    "            df_total = df_total.drop_duplicates(subset=[\"date\", \"ath_name\"], keep=\"first\")\n",
    "            df_total.rename(columns={\"ath_name\": \"name\"})[[\"date\", \"name\", \"ath_country\", \"value\", \"specialty\"]].to_csv(f\"../../web/website/data/rankings/{wf}_{event}_ranking_{season}.csv\", index=False,)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "for country in set(df.ath_country.tolist()):\n",
    "    if country != \"\":\n",
    "        response = requests.get(f\"https://www.countryflags.io/{country}/flat/64.png\")\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
